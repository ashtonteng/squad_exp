{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from utils import DataLoader\n",
    "import re\n",
    "import numpy as np\n",
    "working_dir = os.getcwd()\n",
    "data_dir = os.path.join(working_dir, \"data\")\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension size must be evenly divisible by 10 but is 1 for 'Reshape' (op: 'Reshape') with input shapes: [], [2] and with input tensors computed as partial shapes: input[1] = [?,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    672\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimension size must be evenly divisible by 10 but is 1 for 'Reshape' (op: 'Reshape') with input shapes: [], [2] and with input tensors computed as partial shapes: input[1] = [?,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3a57b9a37846>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   2508\u001b[0m   \"\"\"\n\u001b[0;32m   2509\u001b[0m   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\n\u001b[1;32m-> 2510\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m   2511\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    769\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2338\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1719\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1720\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimension size must be evenly divisible by 10 but is 1 for 'Reshape' (op: 'Reshape') with input shapes: [], [2] and with input tensors computed as partial shapes: input[1] = [?,10]."
     ]
    }
   ],
   "source": [
    "tf.reshape(x, [-1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab files...\n",
      "max word length is 68\n",
      "loading preprocessed data...\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(data_dir=data_dir, embedding_dim=100, batch_size=20, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader.create_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qaIDS, pwords, qwords, pchars, qchars, _, _ = data_loader.next_batch_variable_seq_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qchars[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {\"hello\":3, \"world\":4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max([len(word) for word in d.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_lengths = [len(x) for x in data_loader.para_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(para_lengths, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histedges_equalN(x, nbin):\n",
    "    npt = len(x)\n",
    "    return np.interp(np.linspace(0, npt, nbin + 1),\n",
    "                     np.arange(npt),\n",
    "                     np.sort(x))\n",
    "n, bins, patches = plt.hist(para_lengths, histedges_equalN(para_lengths, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = bins.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "really_discarded_dict = dict()\n",
    "for d in discarded_dict:\n",
    "    qaID = d\n",
    "    paraID, q_words, question, a_words, answer = discarded_dict[qaID]\n",
    "    para_words = para_dict[paraID]\n",
    "    print(\"---\")\n",
    "    try:\n",
    "        i, j = find_sub_list(a_words, para_words)\n",
    "        print(para_words[i:j+1])\n",
    "        print(a_words)\n",
    "    except ValueError:\n",
    "        print(qaID)\n",
    "        really_discarded_dict[qaID] = paraID, q_words, question, a_words, answer\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "really_discarded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_sub_list([\"hey\", \"yolo\"], [\"1\", \"hey\", \"yolo\", \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "def find_exact_sub_list(sublst, lst):\n",
    "    \"\"\"Given a list and a sublist, find the starting and ending indices of the sublist in the list.\"\"\"\n",
    "    for index in (i for i, e in enumerate(lst) if e == sublst[0]): #if list_word == sublist[0]\n",
    "        if lst[index:index+len(sublst)] == sublst:\n",
    "            return index, index + len(sublst) - 1\n",
    "    raise ValueError(\"no exact match found between sublist and list!\")\n",
    "    \n",
    "def find_sub_list(sublst, lst):\n",
    "    try:\n",
    "        return find_exact_sub_list(sublst, lst)\n",
    "    except:\n",
    "        pass\n",
    "    if len(sublst) > 1:\n",
    "        try:\n",
    "            return find_sub_list(sublst[1:], lst)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            return find_sub_list(sublst[1:], lst)\n",
    "        except:\n",
    "            pass\n",
    "    elif len(sublst) == 1:\n",
    "        word = sublst[0]\n",
    "        if bool(re.search(r'\\d', word)): #if word contains a number\n",
    "            try:\n",
    "                number_word = p.number_to_words(word)\n",
    "                return find_sub_list([number_word], lst)\n",
    "            except:\n",
    "                pass\n",
    "        try: #find closest word in paragraph and choose that\n",
    "            idx = np.argmax([difflib.SequenceMatcher(None, word, x).ratio() for x in lst])\n",
    "            return (idx, idx)\n",
    "            #similarities = [difflib.SequenceMatcher(None, word, x).ratio() for x in lst]\n",
    "            #if max(similarities) > 0.4:\n",
    "            #    idx = np.argmax(similarities)\n",
    "            #    return (idx, idx)\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError(\"approximate match between sublist and list not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sublst = ['t', 'parks', ',', 'schools', ',', 'public', 'buildings', ',', 'proper', 'roads', 'and', 'the', 'other', 'amenities', 'that', 'characterise', 'a', 'modern', 'city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = ['the', 'so', '-', 'called', '\"', 'sack', 'of', 'palermo', '\"', 'is', 'one', 'of', 'the', 'major', 'visible', 'faces', 'of', 'the', 'problem', '.', 'the', 'term', 'is', 'used', 'to', 'indicate', 'the', 'speculative', 'building', 'practices', 'that', 'have', 'filled', 'the', 'city', 'with', 'poor', 'buildings', '.', 'the', 'reduced', 'importance', 'of', 'agriculture', 'in', 'the', 'sicilian', 'economy', 'has', 'led', 'to', 'a', 'massive', 'migration', 'to', 'the', 'cities', ',', 'especially', 'palermo', ',', 'which', 'swelled', 'in', 'size', ',', 'leading', 'to', 'rapid', 'expansion', 'towards', 'the', 'north', '.', 'the', 'regulatory', 'plans', 'for', 'expansion', 'was', 'largely', 'ignored', 'in', 'the', 'boom', '.', 'new', 'parts', 'of', 'town', 'appeared', 'almost', 'out', 'of', 'nowhere', ',', 'but', 'without', 'parks', ',', 'schools', ',', 'public', 'buildings', ',', 'proper', 'roads', 'and', 'the', 'other', 'amenities', 'that', 'characterise', 'a', 'modern', 'city', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_approx_sub_list(sublst, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.number_to_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 100\n",
    "batch_size = 10\n",
    "rnn_size = 50\n",
    "W_inputs = tf.constant(np.random.random((rnn_size, rnn_size)), dtype=\"float32\")\n",
    "W_pointerRNN = tf.constant(np.random.random((rnn_size, rnn_size)), dtype=\"float32\")\n",
    "v = tf.constant(np.random.random((rnn_size, 1)), dtype=\"float32\")\n",
    "W_inputs_tiled = tf.tile(tf.expand_dims(W_inputs, 0), [batch_size, 1, 1])\n",
    "#W_pointerRNN_tiled = tf.tile(tf.expand_dims(W_pointerRNN, 0), [batch_size, 1, 1])\n",
    "v_tiled = tf.tile(tf.expand_dims(v, 0), [batch_size, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.random((batch_size, max_seq_length, rnn_size)), dtype=\"float32\")\n",
    "def compute_lengths(inputs):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(inputs), reduction_indices=2))\n",
    "    lengths = tf.reduce_sum(used, reduction_indices=1)\n",
    "    lengths = tf.cast(lengths, tf.int32) #lengths must be integers\n",
    "    return lengths\n",
    "seq_lengths = compute_lengths(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: use attention-pooling over the question representation for the initialize hidden vector of pointer network\n",
    "def loop_fn(time, prev_output, prev_state, prev_loop_state):\n",
    "    print('start loop', time)\n",
    "    next_output = prev_output\n",
    "    if prev_output is None:\n",
    "        next_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        next_state = prev_state\n",
    "    next_loop_state = None\n",
    "    def get_next_input():\n",
    "        print(\"computing logits\")\n",
    "        print(\"prev_output\", prev_output)\n",
    "        if prev_output is None:\n",
    "            logits = tf.matmul(tf.tanh(tf.matmul(inputs, W_inputs_tiled)), v_tiled) #batch_size x seq_length x rnn_size\n",
    "        else:\n",
    "            print(\"i have prev output now\")\n",
    "            logits = tf.matmul(\n",
    "                             tf.tanh(tf.matmul(inputs, W_inputs_tiled) + \n",
    "                                     tf.tile(tf.expand_dims(tf.matmul(prev_output, W_pointerRNN), 1), [1, max_seq_length, 1]))\n",
    "                             , v_tiled) #batch_size x seq_length x rnn_size\n",
    "        print(\"computing probs\")\n",
    "        predicted_probs = tf.nn.softmax(logits, dim=1) #a\n",
    "        print(\"predicted_probs\")\n",
    "        weighted_input = tf.reduce_sum(tf.multiply(predicted_probs, inputs), axis=1) #c #batch_size x rnn_size\n",
    "        print(\"returning weighted_input\")\n",
    "        return weighted_input #weighted input is next input\n",
    "    print(\"1\")\n",
    "    elements_finished = (time >= seq_lengths) #this operation produces boolean tensor of [batch_size] defining if corresponding sequence has ended\n",
    "    print(\"2\")\n",
    "    finished = tf.reduce_all(elements_finished) #AND operation over all batches. True if all batches finished.\n",
    "    time += 1\n",
    "    print(\"3\")\n",
    "    next_input = tf.cond(finished, lambda: tf.zeros([batch_size, rnn_size], dtype=tf.float32), get_next_input)\n",
    "    print(\"4\")\n",
    "    return elements_finished, next_input, next_state, next_output, next_loop_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "    cell = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "    outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "    outputs = outputs_ta.stack()\n",
    "    #predicted_idx = tf.argmax(predicted_probs) #p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.shape(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.random((batch_size, max_seq_length, rnn_size)), dtype=\"float32\")\n",
    "inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_seq_length)\n",
    "inputs_ta = inputs_ta.unstack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ta.size().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ta.read(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.random((max_seq_length, batch_size, rnn_size)), dtype=\"float32\")\n",
    "inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_seq_length)\n",
    "inputs_ta = inputs_ta.unstack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ta.read(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.LSTMCell(rnn_size, reuse=True)\n",
    "outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "outputs = outputs_ta.stack()\n",
    "outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "outputs = outputs_ta.stack()\n",
    "predicted_idx = tf.argmax(self.predicted_probs) #p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: use attention-pooling over the question representation for the initialize hidden vector of pointer network\n",
    "def loop_fn(time, prev_output, prev_state, prev_loop_state):\n",
    "    print('start loop')\n",
    "    next_output = prev_output\n",
    "    if prev_output is None:\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        next_cell_state = prev_state\n",
    "    next_loop_state = None\n",
    "    def get_next_input():\n",
    "        print(\"computing logits\")\n",
    "        print(inputs_ta.read(time))\n",
    "        print(prev_output)\n",
    "        if prev_output is None:\n",
    "            logits = tf.matmul(v, tf.tanh(tf.matmul(inputs_ta.read(time), W_inputs)))\n",
    "        else:\n",
    "            logits = tf.matmul(tf.tanh(tf.matmul(inputs_ta.read(time), W_inputs) + tf.matmul(prev_output, W_pointerRNN)), v) #s #at each step network has access to all paragraph inputs, plus the previous RNN outputs\n",
    "        print(\"computing probs\")\n",
    "        predicted_probs = tf.nn.softmax(logits) #a\n",
    "        print(\"computing weighted_input\")\n",
    "        weighted_input = tf.reduce_sum(tf.multiply(predicted_probs, inputs)) #c\n",
    "        return weighted_input #weighted input is next input\n",
    "    print(\"1\")\n",
    "    elements_finished = (time >= seq_lengths) #this operation produces boolean tensor of [batch_size] defining if corresponding sequence has ended\n",
    "    print(\"2\")\n",
    "    finished = tf.reduce_all(elements_finished) #AND operation over all batches. True if all batches finished.\n",
    "    time += 1\n",
    "    print(\"3\")\n",
    "    next_input = tf.cond(finished, lambda: tf.zeros([batch_size, rnn_size], dtype=tf.float32), get_next_input)\n",
    "    print(\"4\")\n",
    "    return elements_finished, next_input, next_state, next_output, next_loop_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.random((batch_size, max_seq_length, rnn_size)))\n",
    "def compute_lengths(inputs):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(inputs), reduction_indices=2))\n",
    "    lengths = tf.reduce_sum(used, reduction_indices=1)\n",
    "    lengths = tf.cast(lengths, tf.int32) #lengths must be integers\n",
    "    return lengths\n",
    "seq_lengths = compute_lengths(inputs)\n",
    "seq_lengths.eval()\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    print(\"start loop\")\n",
    "    emit_output = cell_output  # == None for time == 0\n",
    "    if cell_output is None:  # time == 0\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        next_cell_state = cell_state\n",
    "    print(\"1\")\n",
    "    elements_finished = (time >= seq_lengths)\n",
    "    print(\"2\")\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    print(\"3\")\n",
    "    next_input = tf.cond(\n",
    "        finished,\n",
    "        lambda: tf.zeros([batch_size, rnn_size], dtype=tf.float32),\n",
    "        lambda: inputs_ta.read(time))\n",
    "    print(\"4\")\n",
    "    next_loop_state = None\n",
    "    print(\"5\")\n",
    "    print(\"elements_finished\", elements_finished)\n",
    "    print(\"next_input\", next_input)\n",
    "    print(\"next_cell_state\", next_cell_state)\n",
    "    print(\"next_loop_state\", next_loop_state)\n",
    "    return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n",
    "\n",
    "max_seq_length = 100\n",
    "batch_size = 10\n",
    "rnn_size = 50\n",
    "W_inputs = tf.constant(np.random.random((rnn_size, rnn_size)))\n",
    "W_pointerRNN = tf.constant(np.random.random((rnn_size, rnn_size)))\n",
    "v = tf.constant(np.random.random((1, rnn_size)))\n",
    "inputs = tf.constant(np.random.random((max_seq_length, batch_size, rnn_size)))\n",
    "inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_seq_length)\n",
    "inputs_ta = inputs_ta.unstack(inputs)\n",
    "cell = tf.contrib.rnn.LSTMCell(rnn_size, reuse=True)\n",
    "outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "outputs = outputs_ta.stack()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
