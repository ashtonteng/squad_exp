{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from utils import DataLoader\n",
    "import re\n",
    "import numpy as np\n",
    "working_dir = os.getcwd()\n",
    "data_dir = os.path.join(working_dir, \"data\")\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab files...\n",
      "max word length is 68\n",
      "loading preprocessed data...\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(data_dir=data_dir, embedding_dim=100, batch_size=20, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.create_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaIDS, pwords, qwords, pchars, qchars, _, _ = data_loader.next_batch_variable_seq_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qchars[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"hello\":3, \"world\":4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(word) for word in d.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_lengths = [len(x) for x in data_loader.para_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(para_lengths, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histedges_equalN(x, nbin):\n",
    "    npt = len(x)\n",
    "    return np.interp(np.linspace(0, npt, nbin + 1),\n",
    "                     np.arange(npt),\n",
    "                     np.sort(x))\n",
    "n, bins, patches = plt.hist(para_lengths, histedges_equalN(para_lengths, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = bins.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "really_discarded_dict = dict()\n",
    "for d in discarded_dict:\n",
    "    qaID = d\n",
    "    paraID, q_words, question, a_words, answer = discarded_dict[qaID]\n",
    "    para_words = para_dict[paraID]\n",
    "    print(\"---\")\n",
    "    try:\n",
    "        i, j = find_sub_list(a_words, para_words)\n",
    "        print(para_words[i:j+1])\n",
    "        print(a_words)\n",
    "    except ValueError:\n",
    "        print(qaID)\n",
    "        really_discarded_dict[qaID] = paraID, q_words, question, a_words, answer\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "really_discarded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sub_list([\"hey\", \"yolo\"], [\"1\", \"hey\", \"yolo\", \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "def find_exact_sub_list(sublst, lst):\n",
    "    \"\"\"Given a list and a sublist, find the starting and ending indices of the sublist in the list.\"\"\"\n",
    "    for index in (i for i, e in enumerate(lst) if e == sublst[0]): #if list_word == sublist[0]\n",
    "        if lst[index:index+len(sublst)] == sublst:\n",
    "            return index, index + len(sublst) - 1\n",
    "    raise ValueError(\"no exact match found between sublist and list!\")\n",
    "    \n",
    "def find_sub_list(sublst, lst):\n",
    "    try:\n",
    "        return find_exact_sub_list(sublst, lst)\n",
    "    except:\n",
    "        pass\n",
    "    if len(sublst) > 1:\n",
    "        try:\n",
    "            return find_sub_list(sublst[1:], lst)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            return find_sub_list(sublst[1:], lst)\n",
    "        except:\n",
    "            pass\n",
    "    elif len(sublst) == 1:\n",
    "        word = sublst[0]\n",
    "        if bool(re.search(r'\\d', word)): #if word contains a number\n",
    "            try:\n",
    "                number_word = p.number_to_words(word)\n",
    "                return find_sub_list([number_word], lst)\n",
    "            except:\n",
    "                pass\n",
    "        try: #find closest word in paragraph and choose that\n",
    "            idx = np.argmax([difflib.SequenceMatcher(None, word, x).ratio() for x in lst])\n",
    "            return (idx, idx)\n",
    "            #similarities = [difflib.SequenceMatcher(None, word, x).ratio() for x in lst]\n",
    "            #if max(similarities) > 0.4:\n",
    "            #    idx = np.argmax(similarities)\n",
    "            #    return (idx, idx)\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError(\"approximate match between sublist and list not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sublst = ['t', 'parks', ',', 'schools', ',', 'public', 'buildings', ',', 'proper', 'roads', 'and', 'the', 'other', 'amenities', 'that', 'characterise', 'a', 'modern', 'city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = ['the', 'so', '-', 'called', '\"', 'sack', 'of', 'palermo', '\"', 'is', 'one', 'of', 'the', 'major', 'visible', 'faces', 'of', 'the', 'problem', '.', 'the', 'term', 'is', 'used', 'to', 'indicate', 'the', 'speculative', 'building', 'practices', 'that', 'have', 'filled', 'the', 'city', 'with', 'poor', 'buildings', '.', 'the', 'reduced', 'importance', 'of', 'agriculture', 'in', 'the', 'sicilian', 'economy', 'has', 'led', 'to', 'a', 'massive', 'migration', 'to', 'the', 'cities', ',', 'especially', 'palermo', ',', 'which', 'swelled', 'in', 'size', ',', 'leading', 'to', 'rapid', 'expansion', 'towards', 'the', 'north', '.', 'the', 'regulatory', 'plans', 'for', 'expansion', 'was', 'largely', 'ignored', 'in', 'the', 'boom', '.', 'new', 'parts', 'of', 'town', 'appeared', 'almost', 'out', 'of', 'nowhere', ',', 'but', 'without', 'parks', ',', 'schools', ',', 'public', 'buildings', ',', 'proper', 'roads', 'and', 'the', 'other', 'amenities', 'that', 'characterise', 'a', 'modern', 'city', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_approx_sub_list(sublst, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.number_to_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 100\n",
    "batch_size = 10\n",
    "rnn_size = 50\n",
    "W_inputs = tf.constant(np.random.random((rnn_size, rnn_size)), dtype=\"float32\")\n",
    "W_pointerRNN = tf.constant(np.random.random((rnn_size, rnn_size)), dtype=\"float32\")\n",
    "v = tf.constant(np.random.random((rnn_size, 1)), dtype=\"float32\")\n",
    "W_inputs_tiled = tf.tile(tf.expand_dims(W_inputs, 0), [batch_size, 1, 1])\n",
    "#W_pointerRNN_tiled = tf.tile(tf.expand_dims(W_pointerRNN, 0), [batch_size, 1, 1])\n",
    "v_tiled = tf.tile(tf.expand_dims(v, 0), [batch_size, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.random((batch_size, max_seq_length, rnn_size)), dtype=\"float32\")\n",
    "def compute_lengths(inputs):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(inputs), reduction_indices=2))\n",
    "    lengths = tf.reduce_sum(used, reduction_indices=1)\n",
    "    lengths = tf.cast(lengths, tf.int32) #lengths must be integers\n",
    "    return lengths\n",
    "seq_lengths = compute_lengths(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: use attention-pooling over the question representation for the initialize hidden vector of pointer network\n",
    "def loop_fn(time, prev_output, prev_state, prev_loop_state):\n",
    "    print('start loop', time)\n",
    "    next_output = prev_output\n",
    "    if prev_output is None:\n",
    "        next_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        next_state = prev_state\n",
    "    next_loop_state = None\n",
    "    def get_next_input():\n",
    "        print(\"computing logits\")\n",
    "        print(\"prev_output\", prev_output)\n",
    "        if prev_output is None:\n",
    "            logits = tf.matmul(tf.tanh(tf.matmul(inputs, W_inputs_tiled)), v_tiled) #batch_size x seq_length x rnn_size\n",
    "        else:\n",
    "            print(\"i have prev output now\")\n",
    "            logits = tf.matmul(\n",
    "                             tf.tanh(tf.matmul(inputs, W_inputs_tiled) + \n",
    "                                     tf.tile(tf.expand_dims(tf.matmul(prev_output, W_pointerRNN), 1), [1, max_seq_length, 1]))\n",
    "                             , v_tiled) #batch_size x seq_length x rnn_size\n",
    "        print(\"computing probs\")\n",
    "        predicted_probs = tf.nn.softmax(logits, dim=1) #a\n",
    "        print(\"predicted_probs\")\n",
    "        weighted_input = tf.reduce_sum(tf.multiply(predicted_probs, inputs), axis=1) #c #batch_size x rnn_size\n",
    "        print(\"returning weighted_input\")\n",
    "        return weighted_input #weighted input is next input\n",
    "    print(\"1\")\n",
    "    elements_finished = (time >= seq_lengths) #this operation produces boolean tensor of [batch_size] defining if corresponding sequence has ended\n",
    "    print(\"2\")\n",
    "    finished = tf.reduce_all(elements_finished) #AND operation over all batches. True if all batches finished.\n",
    "    time += 1\n",
    "    print(\"3\")\n",
    "    next_input = tf.cond(finished, lambda: tf.zeros([batch_size, rnn_size], dtype=tf.float32), get_next_input)\n",
    "    print(\"4\")\n",
    "    return elements_finished, next_input, next_state, next_output, next_loop_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "    cell = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "    outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "    outputs = outputs_ta.stack()\n",
    "    #predicted_idx = tf.argmax(predicted_probs) #p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.shape(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.random((batch_size, max_seq_length, rnn_size)), dtype=\"float32\")\n",
    "inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_seq_length)\n",
    "inputs_ta = inputs_ta.unstack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ta.size().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ta.read(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.random((max_seq_length, batch_size, rnn_size)), dtype=\"float32\")\n",
    "inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_seq_length)\n",
    "inputs_ta = inputs_ta.unstack(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ta.read(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.LSTMCell(rnn_size, reuse=True)\n",
    "outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "outputs = outputs_ta.stack()\n",
    "outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "outputs = outputs_ta.stack()\n",
    "predicted_idx = tf.argmax(self.predicted_probs) #p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: use attention-pooling over the question representation for the initialize hidden vector of pointer network\n",
    "def loop_fn(time, prev_output, prev_state, prev_loop_state):\n",
    "    print('start loop')\n",
    "    next_output = prev_output\n",
    "    if prev_output is None:\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        next_cell_state = prev_state\n",
    "    next_loop_state = None\n",
    "    def get_next_input():\n",
    "        print(\"computing logits\")\n",
    "        print(inputs_ta.read(time))\n",
    "        print(prev_output)\n",
    "        if prev_output is None:\n",
    "            logits = tf.matmul(v, tf.tanh(tf.matmul(inputs_ta.read(time), W_inputs)))\n",
    "        else:\n",
    "            logits = tf.matmul(tf.tanh(tf.matmul(inputs_ta.read(time), W_inputs) + tf.matmul(prev_output, W_pointerRNN)), v) #s #at each step network has access to all paragraph inputs, plus the previous RNN outputs\n",
    "        print(\"computing probs\")\n",
    "        predicted_probs = tf.nn.softmax(logits) #a\n",
    "        print(\"computing weighted_input\")\n",
    "        weighted_input = tf.reduce_sum(tf.multiply(predicted_probs, inputs)) #c\n",
    "        return weighted_input #weighted input is next input\n",
    "    print(\"1\")\n",
    "    elements_finished = (time >= seq_lengths) #this operation produces boolean tensor of [batch_size] defining if corresponding sequence has ended\n",
    "    print(\"2\")\n",
    "    finished = tf.reduce_all(elements_finished) #AND operation over all batches. True if all batches finished.\n",
    "    time += 1\n",
    "    print(\"3\")\n",
    "    next_input = tf.cond(finished, lambda: tf.zeros([batch_size, rnn_size], dtype=tf.float32), get_next_input)\n",
    "    print(\"4\")\n",
    "    return elements_finished, next_input, next_state, next_output, next_loop_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(np.random.random((batch_size, max_seq_length, rnn_size)))\n",
    "def compute_lengths(inputs):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(inputs), reduction_indices=2))\n",
    "    lengths = tf.reduce_sum(used, reduction_indices=1)\n",
    "    lengths = tf.cast(lengths, tf.int32) #lengths must be integers\n",
    "    return lengths\n",
    "seq_lengths = compute_lengths(inputs)\n",
    "seq_lengths.eval()\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    print(\"start loop\")\n",
    "    emit_output = cell_output  # == None for time == 0\n",
    "    if cell_output is None:  # time == 0\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        next_cell_state = cell_state\n",
    "    print(\"1\")\n",
    "    elements_finished = (time >= seq_lengths)\n",
    "    print(\"2\")\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    print(\"3\")\n",
    "    next_input = tf.cond(\n",
    "        finished,\n",
    "        lambda: tf.zeros([batch_size, rnn_size], dtype=tf.float32),\n",
    "        lambda: inputs_ta.read(time))\n",
    "    print(\"4\")\n",
    "    next_loop_state = None\n",
    "    print(\"5\")\n",
    "    print(\"elements_finished\", elements_finished)\n",
    "    print(\"next_input\", next_input)\n",
    "    print(\"next_cell_state\", next_cell_state)\n",
    "    print(\"next_loop_state\", next_loop_state)\n",
    "    return elements_finished, next_input, next_cell_state, emit_output, next_loop_state\n",
    "\n",
    "max_seq_length = 100\n",
    "batch_size = 10\n",
    "rnn_size = 50\n",
    "W_inputs = tf.constant(np.random.random((rnn_size, rnn_size)))\n",
    "W_pointerRNN = tf.constant(np.random.random((rnn_size, rnn_size)))\n",
    "v = tf.constant(np.random.random((1, rnn_size)))\n",
    "inputs = tf.constant(np.random.random((max_seq_length, batch_size, rnn_size)))\n",
    "inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_seq_length)\n",
    "inputs_ta = inputs_ta.unstack(inputs)\n",
    "cell = tf.contrib.rnn.LSTMCell(rnn_size, reuse=True)\n",
    "outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\n",
    "outputs = outputs_ta.stack()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
